---
title: "Problem Set 3"
author: "Pete Cuppernull"
date: "2/11/2020"
output: pdf_document
---

```{r setup, include=TRUE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)
library(tree)
library(ISLR)
library(randomForest)
library(MASS)
library(gbm)
library(rpart)
library(ipred)
```

##Decision Trees

1. Setup
```{r, message=FALSE}
set.seed(1414)
nes <- read_csv(here("data/nes2008.csv"))

p <- nes %>%
  dplyr::select(-biden) %>%
  ncol()

lambda <- seq(0.0001, .04, .001)

```

2. Create Training and Test Sets
```{r}
train2 <- sample(1:nrow(nes), (1807*.75))
##I will use all the rows that are NOT in train2 as my test set later on by subsetting out the training rows
```


##3. Boosting w/ 1000 trees
```{r}
##Run Boost on Training data
boosted_tree_train <- function(lambda){
  model <- gbm(biden ~ .,
    data = nes[train2,],
    distribution="gaussian",
    n.trees=1000,
    shrinkage = lambda)
  
  preds <- predict(model, newdata=nes[train2,], n.trees = 1000)
  
  mse <- mean((nes[train2,]$biden-preds)^2)
  
  mse
}

boost_train_list <- map_dbl(lambda, boosted_tree_train)

boost_train_mses <- as.data.frame(cbind(lambda, boost_train_list)) %>%
  rename(mse = `boost_train_list`)

##Now repeat for test
boosted_tree_test <- function(lambda){
  model <- gbm(biden ~ .,
    data = nes[train2,],
    distribution="gaussian",
    n.trees=1000,
    shrinkage = lambda)
  
  preds <- predict(model, newdata=nes[-train2,], n.trees = 1000)
  
  mse <- mean((nes[-train2,]$biden-preds)^2)
  
  mse
}

boost_test_list <- map_dbl(lambda, boosted_tree_test)

boost_test_mses <- as.data.frame(cbind(lambda, boost_test_list)) %>%
  rename(mse = `boost_test_list`)

ggplot() +
  geom_point(data = boost_train_mses, mapping = aes(lambda, mse), color = "blue") +
  geom_line(data = boost_train_mses, mapping = aes(lambda, mse), color = "blue") +
  geom_point(data = boost_test_mses, mapping = aes(lambda, mse), color = "red") +
  geom_line(data = boost_test_mses, mapping = aes(lambda, mse), color = "red") +
  labs(title = "MSEs Across Different Shrinkage Values",
       y = "MSE",
       x = "Lambda",
       subtitle = "Training in Blue, Testing in Red")

```

##Question 4. Fix Lambda
```{r}
mse_01 <- boosted_tree_test(.01)
```
The test MSE for the boosting procedure with a $\lambda$ of .01 is `r mse_01`. This result is approxiamtely in line with our expected resukts, considering the MSEs for $\lambda$ values of .0091 and .0101 produced in Question 3. The MSE for a $\lambda$ of .01 is slightly higher than these other values, however, highlighting the importance of estimating models with a range of shrinkage values to identify the ideal model.

##Question 5. Apply Bagging
```{r}
biden_bag <- bagging(biden ~ .,
                   data = nes,
                   subset = train2)

  
bag_preds <- predict(biden_bag, newdata=nes[-train2,])
  
bag_mse <- mean((nes[-train2,]$biden-bag_preds)^2)
  
bag_mse

```
The test set MSE for the bagging approach is `r bag_mse`.

##Question 6. Apply Random Forest
```{r}
biden_rf <- randomForest(biden ~ .,
                   data = nes,
                   subset = train2)

  
rf_preds <- predict(biden_rf, newdata=nes[-train2,])
  
rf_mse <- mean((nes[-train2,]$biden-rf_preds)^2)
  
rf_mse
```
The test set MSE for the random forest approach is `r rf_mse`.

##Question 7. Apply Linear Regression
```{r}
biden_lm <- glm(biden ~ .,
                   data = nes,
                   subset = train2)

  
lm_preds <- predict(biden_lm, newdata=nes[-train2,])
  
lm_mse <- mean((nes[-train2,]$biden-lm_preds)^2)
  
lm_mse
```
The test set MSE for linear regression is `r lm_mse`.


##Question 8

The test errors of the different model fits are as follows:

  - Boosted: `r boost_test_mses$mse[which.min(boost_test_mses$mse)]`
  
  - Boosted with .01 $\lambda$: `r boosted_tree_test(.01)`
  
  - Bagging: `r bag_mse`
  
  - Random Forest: `r rf_mse`
  
  - Linear Model: `r lm_mse`
  
The model which generally fits the best is the boosted model with a lambda value of 0.0111. However, it should be noted that the linear model also has a relatively small MSE at `r lm_mse` -- in some cases, it might be preferable to use the linear model and sacrifice a bit of model error for easier interpretation.


#Support Vector Machines
##Question 1. Create Training and Test Sets
```{r}
train_num <- sample(1:nrow(OJ), 800)
oj_train <- OJ[train_num,]
oj_test <- OJ[-train_num,]
```

##Question 2. Fit SV Classifier
```{r, message=FALSE}
library(e1071)
svmfit_oj <- svm(Purchase ~ ., 
             data = oj_train, 
             kernel = "linear", 
             cost = .01)

```
DISCUSS THE RESULTS

##Question 3. Display Confusion Matrix
```{r}
class_pred <- predict(svmfit_oj, oj_test)
confusion_matrix <- table(predicted = class_pred, true = oj_test$Purchase)

test_error <- sum(confusion_matrix[row(confusion_matrix) !=
                                     col(confusion_matrix)]) / sum(confusion_matrix)

class_pred_train <- predict(svmfit_oj, oj_train)
confusion_matrix_train <- table(predicted = class_pred_train, true = oj_train$Purchase)
train_error <- sum(confusion_matrix_train[row(confusion_matrix_train) !=
                                            col(confusion_matrix_train)]) / sum(confusion_matrix_train)

confusion_matrix

```
The training error rate is `r train_error` and the test error rate is `r test_error`.


##Question 4. Find Optimal Cost
```{r}
tune_oj <- tune(svm, 
                Purchase ~ ., 
                data = oj_train, 
                kernel = "linear", 
                ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000)))

# best?
tuned_model <- tune_oj$best.model
summary(tuned_model)

##Let's re-tune around the .1 value
tune_oj2 <- tune(svm, 
                Purchase ~ ., 
                data = oj_train, 
                kernel = "linear", 
                ranges = list(cost = seq(.01, .4, .01)))

tuned_model2 <- tune_oj2$best.model

```
The optimal cost of the model is `r tuned_model2$cost`.


##Question 5. Compute New Training and Test Error Rates for New Cost
```{r}
svmfit_oj_tuned_cost <- svm(Purchase ~ ., 
             data = oj_train, 
             kernel = "linear", 
             cost = .04)


class_pred_tuned <- predict(svmfit_oj_tuned_cost, oj_test)
confusion_matrix_tuned <- table(predicted = class_pred_tuned, true = oj_test$Purchase)

test_error_tuned <- sum(confusion_matrix_tuned[row(confusion_matrix_tuned) != col(confusion_matrix_tuned)]) / sum(confusion_matrix_tuned)

class_pred_train_tuned <- predict(svmfit_oj_tuned_cost, oj_train)

confusion_matrix_train_tuned <- table(predicted = class_pred_train_tuned, true = oj_train$Purchase)

train_error_tuned <- sum(confusion_matrix_train_tuned[row(confusion_matrix_train_tuned) != col(confusion_matrix_train_tuned)]) / sum(confusion_matrix_train_tuned)

confusion_matrix_tuned
```

The training set error rate for the tuned model is `r train_error_tuned` and the test set error rate for the tuned model is `r test_error_tuned`. This is slightly better than the naive model with a cost of .01, which had training and test error rates of `r train_error` and `r test_error`, respectively. In substantive terms, this difference is quite minimal -- the tuned model correctly classified exactly one observation more than the naive model for the test set (out of  270 observations). It did, however, classify eight more observations correctly than the naive model for the training set. While the performance of the tuned classifier is, at best, marginally better than the naive classifier, I would argue that it is still better to leverage the tuned model because of the minimal resources and time required to acheive a slightly better predictor.
