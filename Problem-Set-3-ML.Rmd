---
title: "Problem Set 3"
author: "Pete Cuppernull"
date: "2/11/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)
library(tree)
library(ISLR)
library(randomForest)
library(MASS)
library(gbm)
library(rpart)
library(ipred)
```

##Decision Trees

1. Setup
```{r}
set.seed(1414)
nes <- read_csv(here("data/nes2008.csv"))

p <- nes %>%
  dplyr::select(-biden) %>%
  ncol()

lambda <- seq(0.0001, .04, .001)

```

2. Create Training and Test Sets
```{r}
nes
nes_rows <- as.data.frame(seq(1, 1807, 1)) %>%
  rename(all_rows = `seq(1, 1807, 1)`)
train <- as.data.frame(sample(1:1807, (1807*.75)))

train_test_sets <- train %>%
  rename(row = `sample(1:1807, (1807 * 0.75))`) %>%
  arrange(row) %>%
  mutate(set = "train") %>%
  right_join(nes_rows, by = c("row" = "all_rows")) %>%
  mutate(set = if_else(is.na(set), "test", set))

train2 <- sample(1:nrow(nes), (1807*.75))
str(train2)
head(train_test_sets)
```


##3. Boosting w/ 1000 trees
```{r}
nes_rf <- randomForest(biden ~ .,
                   data = nes,
                   subset = train2)

##Run Boost on Training data
boosted_tree_train <- function(lambda){
  model <- gbm(biden ~ .,
    data = nes[train2,],
    distribution="gaussian",
    n.trees=1000,
    shrinkage = lambda)
  
  preds <- predict(model, newdata=nes[train2,], n.trees = 1000)
  
  mse <- mean((nes[train2,]$biden-preds)^2)
  
  mse
}

boost_train_list <- map_dbl(lambda, boosted_tree_train)

boost_train_mses <- as.data.frame(cbind(lambda, boost_train_list)) %>%
  rename(mse = `boost_train_list`)

##Now repeat for test
boosted_tree_test <- function(lambda){
  model <- gbm(biden ~ .,
    data = nes[train2,],
    distribution="gaussian",
    n.trees=1000,
    shrinkage = lambda)
  
  preds <- predict(model, newdata=nes[-train2,], n.trees = 1000)
  
  mse <- mean((nes[-train2,]$biden-preds)^2)
  
  mse
}

boost_test_list <- map_dbl(lambda, boosted_tree_test)

boost_test_mses <- as.data.frame(cbind(lambda, boost_test_list)) %>%
  rename(mse = `boost_test_list`)

boost_test_mses$lambda[which.min(boost_test_mses$mse)]

ggplot() +
  geom_point(data = boost_train_mses, mapping = aes(lambda, mse), color = "blue") +
  geom_line(data = boost_train_mses, mapping = aes(lambda, mse), color = "blue") +
  geom_point(data = boost_test_mses, mapping = aes(lambda, mse), color = "red") +
  geom_line(data = boost_test_mses, mapping = aes(lambda, mse), color = "red") +
  labs(title = "MSEs Across Different Shrinkage Values",
       y = "MSE",
       x = "Lambda",
       subtitle = "Training in Blue, Testing in Red")

```

##Question 4. Fix Lambda
```{r}
boosted_tree_test(.01)
```

##Question 5. Apply Bagging
```{r}
biden_bag <- bagging(biden ~ .,
                   data = nes,
                   subset = train2)

  
bag_preds <- predict(biden_bag, newdata=nes[-train2,])
  
bag_mse <- mean((nes[-train2,]$biden-bag_preds)^2)
  
bag_mse

```

##Question 6. Apply Random Forest
```{r}
biden_rf <- randomForest(biden ~ .,
                   data = nes,
                   subset = train2)

  
rf_preds <- predict(biden_rf, newdata=nes[-train2,])
  
rf_mse <- mean((nes[-train2,]$biden-rf_preds)^2)
  
rf_mse
```

##Question 7. Apply Linear Regression
```{r}
biden_lm <- glm(biden ~ .,
                   data = nes,
                   subset = train2)

  
lm_preds <- predict(biden_lm, newdata=nes[-train2,])
  
lm_mse <- mean((nes[-train2,]$biden-lm_preds)^2)
  
lm_mse
```

##Question 8

The test errors of the different model fits are as follows:

  - Boosted: `r boost_test_mses$mse[which.min(boost_test_mses$mse)]`
  
  - Boosted with .01 $\lambda$: `r boosted_tree_test(.01)`
  
  - Bagging: `r bag_mse`
  
  - Random Forest: `r rf_mse`
  
  - Linear Model: `r lm_mse`
  
The model which generally fits the best is the boosted model with a lambda value of 0.0111. However, it should be noted that the linear model also has a relatively small MSE at `r lm_mse` -- in some cases, it might be preferable to use the linear model and sacrifice a bit of model error for easier interpretation.


#Support Vector Machines
##Question 1. Create Training and Test Sets
```{r}
train_num <- sample(1:nrow(OJ), 800)
oj_train <- OJ[train_num,]
oj_test <- OJ[-train_num,]
```

##Question 2. Fit SV Classifier
```{r}
library(e1071)
svmfit_oj <- svm(Purchase ~ ., 
             data = oj_train, 
             kernel = "linear", 
             cost = .01)

summary(svmfit_oj)
```


##Question 3. Display Confusion Matrix
```{r}
class_pred <- predict(svmfit_oj, oj_test)
confusion_matrix <- table(predicted = class_pred, true = oj_test$Purchase)

test_error <- sum(confusion_matrix[row(confusion_matrix) != col(confusion_matrix)]) / sum(confusion_matrix)

class_pred_train <- predict(svmfit_oj, oj_train)
confusion_matrix_train <- table(predicted = class_pred_train, true = oj_train$Purchase)
train_error <- sum(confusion_matrix_train[row(confusion_matrix_train) != col(confusion_matrix_train)]) / sum(confusion_matrix_train)

```

##Question 4. Find Optimal Cost
```{r}
tune_oj <- tune(svm, 
                Purchase ~ ., 
                data = oj_train, 
                kernel = "linear", 
                ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100, 1000)))

# CV errors
summary(tune_c)

# best?
tuned_model <- tune_c$best.model
summary(tuned_model)
```


